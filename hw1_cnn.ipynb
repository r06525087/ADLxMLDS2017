{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.layers import Concatenate,Masking,Add,concatenate, UpSampling1D,MaxPooling1D,Input,Dense,LSTM,TimeDistributed,Activation,Dropout,BatchNormalization,Conv1D,Bidirectional\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.layers.core import RepeatVector\n",
    "from keras.layers import convolutional\n",
    "\n",
    "from keras.models import load_model\n",
    "model = load_model('cnn_model.h5')\n",
    "\n",
    "\n",
    "import sys\n",
    "path_1 = sys.argv[1]\n",
    "path_2 = sys.argv[2]\n",
    "\n",
    "input_time_step = 36\n",
    "\n",
    "file = open(path_1+'mfcc/test.ark')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_mfcc=[]\n",
    "for line in file:\n",
    "    test_mfcc.append(line)\n",
    "\n",
    "for i in range (0,len(test_mfcc)):\n",
    "    test_mfcc[i]=test_mfcc[i].split()\n",
    "for i in range (0,len(test_mfcc)):\n",
    "    temp=['s',[]]\n",
    "\n",
    "    for k in range(1,40):\n",
    "        temp[0]=test_mfcc[i][0]\n",
    "        temp[1].append(test_mfcc[i][k])\n",
    "    test_mfcc[i]=temp\n",
    "for i in range (0,len(test_mfcc)):\n",
    "    for k in range (0,39):\n",
    "        test_mfcc[i][1][k]=float(test_mfcc[i][1][k])\n",
    "\n",
    "for i in range (0,len(test_mfcc)):\n",
    "    test_mfcc[i][0]=test_mfcc[i][0].replace('_',' ')\n",
    "for i in range (0,len(test_mfcc)):\n",
    "    test_mfcc[i][0]=test_mfcc[i][0].split()\n",
    "for i in range (0,len(test_mfcc)):\n",
    "    test_mfcc[i][0]=[test_mfcc[i][0][0]+test_mfcc[i][0][1],test_mfcc[i][0][2]]\n",
    "for i in range (0,len(test_mfcc)):\n",
    "    test_mfcc[i][0][1]=int(test_mfcc[i][0][1])\n",
    "\n",
    "process_test_mfcc=[]\n",
    "temp=[]\n",
    "for i in range (0,len(test_mfcc)):\n",
    "    if test_mfcc[i][0][1]==1:\n",
    "        process_test_mfcc.append(temp)\n",
    "        \n",
    "        temp=['name',0]\n",
    "        temp[0]=test_mfcc[i][0][0]\n",
    "        temp.append(test_mfcc[i][1])\n",
    "    else:\n",
    "        temp.append(test_mfcc[i][1]) \n",
    "        \n",
    "        \n",
    "process_test_mfcc.append(temp)\n",
    "process_test_mfcc.remove([])\n",
    "\n",
    "#應該會長得像 [ ['名字',int(向量個數),[vector_1],[vector_2]......[vector_n],.. ] , [...] , [...] ...]\n",
    "\n",
    "process_test_mfcc_2=[]\n",
    "for i in range(0, len(process_test_mfcc)):\n",
    "    temp = [[],[]]\n",
    "    temp[0].append(process_test_mfcc[i][0])\n",
    "    temp[0].append(len(process_test_mfcc[i])-2)\n",
    "    for k in range(2,len(process_test_mfcc[i])):\n",
    "        temp[1].append(process_test_mfcc[i][k])\n",
    "    \n",
    "    process_test_mfcc_2.append(temp)\n",
    "#變成 [['名字',向量數],[ 好多個向量list在這   ]]\n",
    "\n",
    "# 寫函數 : input = 標準資料格式 \n",
    "#         output = 1個list包含n個人的SEQ, 每個人的SEQ由seq_len倍數的切開, 不足補0向量\n",
    "\n",
    "def seperate_test_data(input_data_2,seq_len,fill_thing):\n",
    "    \n",
    "    return_list = []\n",
    "    \n",
    "    \n",
    "    name_list=[]\n",
    "    \n",
    "    for i in range(0,len(input_data_2)):\n",
    "        name_list.append(input_data_2[i][0])\n",
    "    \n",
    "    \n",
    "\n",
    "    data_list = []\n",
    "    \n",
    "    for i in range(0,len(input_data_2)):\n",
    "        \n",
    "        one_sentence = []\n",
    "        phone_num = (int( len(input_data_2[i][1])/seq_len ) +1)*seq_len #計算每個句子補完後該有的長度\n",
    "        \n",
    "        for k in range(0,phone_num-len(input_data_2[i][1])):\n",
    "            input_data_2[i][1].append(fill_thing)  #長度不夠的部分由fill_thing補完\n",
    "        \n",
    "        for j in range(0,int(phone_num/seq_len)):\n",
    "            part_list = []                         #準備整串PHONE/SEQ_LEN數量的LIST\n",
    "            part_list = input_data_2[i][1][seq_len*j:seq_len*(j+1)]\n",
    "        \n",
    "            one_sentence.append(part_list)\n",
    "        \n",
    "        data_list.append(one_sentence)\n",
    "    \n",
    "    return_list.append(name_list)\n",
    "    return_list.append(data_list)\n",
    "    \n",
    "    return return_list\n",
    "        \n",
    "        \n",
    "\n",
    "process_test_mfcc_2 = seperate_test_data(process_test_mfcc_2,input_time_step,np.zeros(39))\n",
    "\n",
    "# input = 經過切開及fill的資料\n",
    "# output = 一個大list 每個元素為 [[id],[預測出的結果還原回1~39的數字]]\n",
    "\n",
    "def evaluate_all(input_list):\n",
    "    return_list = []\n",
    "    \n",
    "    for i in range(0,len(input_list[1])):\n",
    "        one_person_data = []\n",
    "        \n",
    "        one_person_data.append(input_list[0][i][0]) #小list的第一筆是第i筆資料的人名\n",
    "\n",
    "        i_predict = model.predict(x=np.array(input_list[1][i])) #將第i筆資料的data取出來預測\n",
    "        i_predict = list(i_predict)\n",
    "        \n",
    "        predict_list = [] #用來裝預測完的東西\n",
    "        \n",
    "        for m in range(0,len(i_predict)):\n",
    "            for n in range(0,len(i_predict[0])):\n",
    "                predict_list.append(list(i_predict[m][n]))\n",
    "            \n",
    "        \n",
    "        phone_number=[]                        #用來裝一串數字\n",
    "        \n",
    "        for k in range(0,input_list[0][i][1]): #k是實際有的PHONE數量 所以只做K次\n",
    "            \n",
    "            idx = predict_list[k].index(max(predict_list[k]))\n",
    "            \n",
    "            phone_number.append(idx)\n",
    "            \n",
    "        \n",
    "        one_person_data.append(phone_number)\n",
    "        \n",
    "        return_list.append(one_person_data)\n",
    "    \n",
    "    return return_list\n",
    "\n",
    "result=evaluate_all(process_test_mfcc_2)\n",
    "\n",
    "for i in range(0,len(result)):\n",
    "    for k in range(0,len(result[i][1])):\n",
    "        if result[i][1][k]==39:\n",
    "\n",
    "            result[i][1][k]=30\n",
    "\n",
    "\n",
    "\n",
    "def delete_sil_continue(input_result):\n",
    "    for i in range(len(input_result)):\n",
    "\n",
    "        data_list=[]\n",
    "        data_list.append(input_result[i][1][0])\n",
    "    \n",
    "        for k in range(0,len(input_result[i][1])):\n",
    "            if input_result[i][1][k]!=data_list[len(data_list)-1]:\n",
    "                data_list.append(input_result[i][1][k])\n",
    "        \n",
    "        if data_list[0]==30:\n",
    "            del(data_list[0])\n",
    "        if data_list[-1]==30:\n",
    "            del(data_list[-1])\n",
    "        \n",
    "        input_result[i][1]=data_list\n",
    "\n",
    "delete_sil_continue(result)\n",
    "\n",
    "\n",
    "num_to_phone={0:'aa',  1:'ae',  2:'ah',  3:'aw',  4:'ay',  5:'b',  6:'ch',  7:'d',  8:'dh',  9:'dx',  \n",
    "              10:'eh',11:'er', 12:'ey',  13:'f',  14:'g',15:'hh', 16:'ih',17:'iy', 18:'jh',  19:'k',  \n",
    "               20:'l', 21:'m',  22:'n', 23:'ng', 24:'ow',25:'oy',  26:'p', 27:'r',  28:'s', 29:'sh',\n",
    "             30:'sil', 31:'t' ,32:'th', 33:'uh', 34:'uw', 35:'v',  36:'w', 37:'y',  38:'z'}\n",
    "\n",
    "final_diction={'aa':'a', 'ae':'b', 'ah':'c', 'aw':'e', 'ay':'g', 'b':'h', 'ch':'i', 'd':'k', 'dh':'l', 'dx':'m',\n",
    "               'eh':'n', 'er':'r', 'ey':'s', 'f':'t',  'g':'u',  'hh':'v','ih':'w', 'iy':'y','jh':'z', 'k':'A',\n",
    "               'l' :'B', 'm':'C',  'n':'D',  'ng':'E', 'ow':'F', 'oy':'G','p':'H',  'r':'I', 's':'J',  'sh':'K',\n",
    "               'sil':'L','t':'M',  'th':'N', 'uh':'O', 'uw':'P', 'v':'Q', 'w':'S',  'y':'T', 'z':'U'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def turn_num_to_phone(input_result):\n",
    "    final_result=[]\n",
    "    \n",
    "    for i in range(0,len(input_result)):\n",
    "        the_result=[]\n",
    "       \n",
    "        name = input_result[i][0]\n",
    "        name = name[0:5]+'_'+name[5:]\n",
    "        \n",
    "        the_result.append(name) #名字\n",
    "\n",
    "       \n",
    "        input_result[i][1]=[num_to_phone[x] if x in num_to_phone else x for x in input_result[i][1]] \n",
    "        input_result[i][1]=[final_diction[x] if x in final_diction else x for x in input_result[i][1]]\n",
    "        \n",
    "        first_phone=input_result[i][1][0]\n",
    "        \n",
    "        for k in range(1,len(input_result[i][1])):\n",
    "            first_phone = first_phone+input_result[i][1][k]\n",
    "        \n",
    "        the_result.append(first_phone)\n",
    "    \n",
    "        final_result.append(the_result)\n",
    "    \n",
    "    return final_result\n",
    "\n",
    "OK_result = turn_num_to_phone(result) \n",
    "\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "with open(path_2+'.csv', mode='w', encoding='utf-8') as write_file:\n",
    "    writer = csv.writer(write_file, delimiter=',')\n",
    "\n",
    "\n",
    "    writer.writerow(['id','phone_sequence'])\n",
    "    for i in range (0,len(OK_result)):\n",
    "        writer.writerow(OK_result[i])\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "###  資料前處理跟 BEST 裡面都一樣  模型不同~~      \n",
    "        \n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(BatchNormalization(input_shape=(input_time_step,39)))\n",
    "\n",
    "# model.add(Conv1D(filters=64,\n",
    "#                  kernel_size=7,\n",
    "#                  padding='same',\n",
    "#                  strides=1\n",
    "#                  ))\n",
    "# model.add(LeakyReLU())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Conv1D(filters=128,\n",
    "#                  kernel_size=7,\n",
    "#                  padding='same',\n",
    "#                  strides=1\n",
    "#                  ))\n",
    "# model.add(LeakyReLU())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Conv1D(filters=128,\n",
    "#                  kernel_size=7,\n",
    "#                  padding='same',\n",
    "#                  strides=1\n",
    "#                  ))\n",
    "# model.add(LeakyReLU())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "# model.add(Bidirectional(LSTM(units=128,return_sequences=True)))\n",
    "# model.add(LeakyReLU())\n",
    "# model.add(Dropout(0.2))\n",
    "          \n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "# model.add(Bidirectional(LSTM(units=256,return_sequences=True)))\n",
    "# model.add(LeakyReLU())\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "# model.add(TimeDistributed(Dense(40)))\n",
    "# model.add(LeakyReLU())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation('softmax'))\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adam',\n",
    "#               metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
